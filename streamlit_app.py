import os
import json
import streamlit as st
import openai # Aunque 'OpenAI' se usa despu√©s, 'openai' puede ser para otras cosas o un vestigio.
from dotenv import load_dotenv
import re
from openai import OpenAI # Import espec√≠fico para el cliente
import geopandas as gpd # No usado en el fragmento, pero puede ser para funcionalidades futuras
import folium # No usado en el fragmento, pero puede ser para funcionalidades futuras
from streamlit_folium import st_folium # No usado en el fragmento, pero puede ser para funcionalidades futuras
import unicodedata
import pandas as pd # Necesario para pd.isna en normalize_text

# Cargar variables de entorno (si se usa .env localmente)
load_dotenv()

client = OpenAI(
  api_key=st.secrets['openai_key'],
)

# Configuraci√≥n de Streamlit
st.set_page_config(page_title="Chatbot Corrupci√≥n üí¨", layout="centered")

with st.sidebar:
    st.image(".streamlit/logo.png", use_container_width=True)
    st.title('Chatbot Corrupci√≥n')
    st.markdown('''
    ## Sobre este Chatbot

    Bienvenido al **Chatbot Corrupci√≥n**, una herramienta interactiva dise√±ada para facilitar el acceso y comprensi√≥n de los informes de auditor√≠a relacionados con casos de corrupci√≥n en los gobiernos subnacionales del Per√∫ durante el per√≠odo **2016-2022**.

    Nuestra base de datos incluye **todos los informes de control** emitidos por la **Contralor√≠a General de la Rep√∫blica del Per√∫** en esos a√±os, proporcionando una cobertura completa y actualizada de las acciones de control realizadas a nivel nacional.

    Utiliza este chatbot para explorar informaci√≥n detallada sobre auditor√≠as, hallazgos y recomendaciones, y para obtener respuestas precisas basadas en los documentos oficiales.

    ---
    ### C√≥mo utilizar este Chatbot
    - **Realiza preguntas claras y espec√≠ficas** sobre los informes de auditor√≠a.
    - **Indica localidades o per√≠odos de inter√©s** para obtener informaci√≥n detallada.
    - **Recuerda que las respuestas se basan en documentos oficiales**, y si no se dispone de cierta informaci√≥n, se te proporcionar√° orientaci√≥n para obtenerla.

    ---
    ''')
    st.markdown('Desarrollado por **Q-Lab** - Laboratorio de Inteligencia Artificial y M√©todos Computacionales en Ciencias Sociales ([qlab.pucp.edu.pe](https://qlab.pucp.edu.pe/))')
    st.markdown('Contacto: ‚úâÔ∏è [qlab_csociales@pucp.edu.pe](mailto:qlab_csociales@pucp.edu.pe)')

    if st.button("üóëÔ∏è Limpiar conversaci√≥n"):
        st.session_state.messages = [{"role": "assistant", "content": "Conversaci√≥n reiniciada. ¬øEn qu√© m√°s puedo ayudarte?"}]
        st.experimental_rerun() # st.rerun() en versiones m√°s nuevas


def load_chunks_from_jsonl(input_file='salida_chunks_final.jsonl'):
    docs_chunks_list = []
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    chunk = json.loads(line.strip())
                    docs_chunks_list.append(chunk)
                except json.JSONDecodeError as e:
                    print(f"Advertencia: Omitiendo l√≠nea malformada en '{input_file}': {line.strip()}")
                    print(f"Error de decodificaci√≥n: {e}")
                    continue
    except FileNotFoundError:
        print(f"Error: El archivo '{input_file}' no fue encontrado.")
        return []
    except Exception as e:
        print(f"Ocurri√≥ un error inesperado al leer '{input_file}': {e}")
        return []
    return docs_chunks_list

# Carga de datos (global para la app)
docs_chunks = load_chunks_from_jsonl('salida_chunks_final.jsonl')

system_prompt_v2 = """
Eres un asistente virtual experto en analizar y resumir informes de auditor√≠a de la Contralor√≠a General de la Rep√∫blica del Per√∫, enfocados en la gesti√≥n de gobiernos subnacionales durante el per√≠odo 2016-2022. Tu principal tarea es ayudar a los usuarios a entender la situaci√≥n de la gesti√≥n p√∫blica y los hallazgos relevantes, incluyendo aquellos que podr√≠an indicar irregularidades o corrupci√≥n.

**Principios Clave para tus Respuestas:**
1.  **Basado en Evidencia:** Responde √öNICAMENTE con informaci√≥n extra√≠da de los chunks de los informes de auditor√≠a proporcionados en el contexto. No inventes informaci√≥n ni hagas suposiciones m√°s all√° de lo escrito.
2.  **Referencia Expl√≠cita:** SIEMPRE que utilices informaci√≥n de un informe, comienza tu respuesta o el p√°rrafo relevante mencionando el n√∫mero de informe. Ejemplo: "Seg√∫n el informe 'NRO-INFORME-A√ëO', se observ√≥ que..." o "El informe 'NRO-INFORME-A√ëO' detalla lo siguiente:..."
3.  **Precisi√≥n y Detalle:** S√© preciso y, cuando se soliciten detalles o res√∫menes, incluye la informaci√≥n relevante como entidades auditadas, montos involucrados (si los hay en el chunk), principales hallazgos (observaciones), y recomendaciones clave.
4.  **Neutralidad:** Presenta los hechos tal como est√°n en los informes. Aunque los usuarios puedan preguntar sobre "corrupci√≥n", los informes detallan "observaciones" o "irregularidades". Utiliza esa terminolog√≠a, pero entiende que el usuario se refiere a esos hallazgos.
5.  **Manejo de Informaci√≥n Faltante:**
    *   Si no tienes informaci√≥n para una localidad Y per√≠odo espec√≠fico, PERO tienes informaci√≥n para esa localidad en OTROS per√≠odos, o para esa regi√≥n en el per√≠odo solicitado, ind√≠calo claramente. Ejemplo: "No tengo informes espec√≠ficos para [Distrito X] en [A√±o Y]. Sin embargo, para [Distrito X] en [A√±o Z] el informe '[NRO-INFORME]' se√±ala... Y para la regi√≥n de [Regi√≥n W] en [A√±o Y], el informe '[NRO-INFORME]' indica..."
    *   Si no tienes absolutamente ninguna informaci√≥n relevante para la consulta, responde: "No dispongo de informaci√≥n sobre [tema de la consulta]. Para m√°s detalles, por favor consulte directamente con la Contralor√≠a General de la Rep√∫blica del Per√∫."

**Instrucciones Espec√≠ficas para Tipos de Preguntas:**

**A. Para "Formular informes" o "Resumir situaci√≥n" por a√±o y regi√≥n/localidad:**
    *   Cuando se te pida un resumen o "informe" para un **a√±o y una regi√≥n espec√≠ficos**:
        1.  Identifica todos los chunks relevantes proporcionados en el contexto que coincidan con esos criterios (puedes guiarte por los metadatos del chunk si estuvieran disponibles en el contexto, o por la informaci√≥n textual).
        2.  Sintetiza la informaci√≥n de estos chunks.
        3.  Estructura tu respuesta de la siguiente manera (si es posible y la informaci√≥n lo permite):
            *   "Resumen de hallazgos para [Localidad/Regi√≥n] en el a√±o [A√±o]:"
            *   Para cada informe relevante encontrado:
                *   "**Informe [NRO-INFORME-A√ëO] (Entidad: [ENTIDAD_AUDITADA]):**"
                *   "   **Objetivo Principal de la Auditor√≠a:** [Si est√° disponible en el chunk de objetivo]"
                *   "   **Principales Observaciones/Hallazgos:**"
                *   "      - [Resumen de la observaci√≥n 1 del informe, mencionando montos si son relevantes y est√°n en el chunk]"
                *   "      - [Resumen de la observaci√≥n 2 del informe, etc.]"
                *   "   **Recomendaciones Clave:**"
                *   "      - [Resumen de la recomendaci√≥n 1 del informe]"
                *   "      - [Resumen de la recomendaci√≥n 2 del informe, etc.]"
                *   "   **Posibles Implicancias (si se mencionan en los metadatos o el texto del chunk de observaci√≥n):** [Ej: Responsabilidad Penal, Administrativa, Perjuicio Econ√≥mico de S/ XXX]"
            *   Si hay m√∫ltiples informes, pres√©ntalos secuencialmente.
            *   Finaliza con un breve resumen general si puedes identificar patrones o temas comunes entre los informes de esa localidad/a√±o.
    *   Si no hay informes para la combinaci√≥n exacta, sigue la pol√≠tica de manejo de informaci√≥n faltante (Principio Clave 5).

**B. Para responder sobre la "situaci√≥n de la corrupci√≥n" o "hallazgos de corrupci√≥n" en a√±os y regiones espec√≠ficas:**
    *   Aplica la misma l√≥gica que en el punto A, pero enfoca tu resumen en las "Observaciones" y las implicancias de responsabilidad (penal, administrativa, perjuicio econ√≥mico) que encuentres en los chunks.
    *   Interpreta "corrupci√≥n" como las irregularidades, observaciones y hallazgos detallados en los informes.
    *   S√© claro al presentar los hechos: "El informe X identific√≥ las siguientes observaciones que podr√≠an ser de su inter√©s respecto a irregularidades en la gesti√≥n..."

**C. Para preguntas sobre un informe espec√≠fico (por n√∫mero de informe):**
    *   Si el usuario pregunta por un n√∫mero de informe espec√≠fico, y tienes chunks de ese informe en el contexto:
        1.  Presenta el t√≠tulo del informe.
        2.  Menciona la entidad auditada, per√≠odo auditado y fecha de emisi√≥n.
        3.  Resume el objetivo general (si est√° disponible).
        4.  Detalla TODAS las observaciones proporcionadas en los chunks de ese informe, incluyendo montos y responsabilidades si se especifican.
        5.  Detalla TODAS las recomendaciones proporcionadas en los chunks de ese informe.
        6.  No omitas detalles relevantes que est√©n en los chunks del contexto para ese informe.

**Consideraciones Adicionales:**
*   **Concisi√≥n y Relevancia:** Aunque se pide ser completo, evita la verbosidad innecesaria. Prioriza la informaci√≥n que directamente responde a la pregunta del usuario.
*   **Tono Profesional:** Mant√©n un tono formal e informativo, como corresponde a un experto en auditor√≠a.
*   **Limitaci√≥n de Conocimiento:** Reitera que tu conocimiento se basa *exclusivamente* en los documentos que se te proporcionan en el contexto para cada consulta.
"""

def normalize_text(text):
    """Normaliza el texto a min√∫sculas y quita tildes."""
    if pd.isna(text) or not text: # Manejo de NaN y strings vac√≠os
        return ""
    text = str(text).lower()
    nfkd_form = unicodedata.normalize('NFKD', text)
    return "".join([c for c in nfkd_form if not unicodedata.combining(c)])

# --- ESTA ES LA VERSI√ìN DE extract_query_parameters QUE SE CONSERVA ---
def extract_query_parameters(question):
    """
    Extrae a√±o(s), REGIONES y palabras clave de la pregunta del usuario.
    Retorna un diccionario con los par√°metros encontrados.
    """
    params = {
        "years": [],
        "regions": [], # Solo regiones ahora
        "keywords": [],
        "is_specific_enough": False
    }

    normalized_question = normalize_text(question)
    if not normalized_question:
        return params

    params["years"] = list(set(re.findall(r'\b(201[6-9]|202[0-2])\b', normalized_question)))
    if params["years"]:
        params["is_specific_enough"] = True

    known_regions = [
        "amazonas", "ancash", "apurimac", "arequipa", "ayacucho", "cajamarca",
        "callao", "cusco", "huancavelica", "huanuco", "ica", "junin",
        "la libertad", "lambayeque", "lima", "loreto", "madre de dios",
        "moquegua", "pasco", "piura", "puno", "san martin", "tacna",
        "tumbes", "ucayali"
    ]

    temp_question_for_keywords = normalized_question
    found_locations_in_query = []
    words = re.findall(r'\b\w+\b', normalized_question)

    for n in range(3, 0, -1):
        ngrams = [" ".join(words[i:i+n]) for i in range(len(words)-n+1)]
        for ngram_candidate in ngrams:
            already_processed = False
            for found_loc in found_locations_in_query:
                if ngram_candidate in found_loc and ngram_candidate != found_loc:
                    already_processed = True
                    break
            if already_processed:
                continue

            if ngram_candidate in known_regions:
                if ngram_candidate not in params["regions"]:
                    params["regions"].append(ngram_candidate)
                if ngram_candidate not in found_locations_in_query:
                    is_substring_of_existing = any(ngram_candidate in existing_loc and ngram_candidate != existing_loc for existing_loc in found_locations_in_query)
                    if not is_substring_of_existing:
                        found_locations_in_query = [loc for loc in found_locations_in_query if loc not in ngram_candidate]
                        found_locations_in_query.append(ngram_candidate)
                params["is_specific_enough"] = True

    found_locations_in_query = params["regions"] # Asegurar que solo use las regiones a√±adidas

    for year_found in params["years"]:
        temp_question_for_keywords = re.sub(r'\b' + re.escape(year_found) + r'\b', '', temp_question_for_keywords)
    for region_found in found_locations_in_query:
        temp_question_for_keywords = re.sub(r'\b' + re.escape(region_found) + r'\b', '', temp_question_for_keywords)

    stopwords = [
        "de", "la", "el", "en", "y", "o", "del", "los", "las", "un", "una", "unos", "unas",
        "sobre", "acerca", "informe", "reporte", "situacion", "caso", "casos", "corrupcion",
        "auditoria", "contraloria", "gobierno", "municipalidad", "region", "provincia", "distrito",
        "general", "republica", "peru", "quiero", "saber", "dime", "podrias", "informacion",
        "detalles", "cual", "cuales", "como", "cuando", "donde", "que", "quien", "porque",
        "mas", "menos", "todo", "todos", "entidad", "publico", "publica"
    ]
    params["keywords"] = [
        kw for kw in re.findall(r'\b[a-z]{3,}\b', temp_question_for_keywords)
        if kw not in stopwords and kw not in params["years"] and kw not in found_locations_in_query
    ]

    if not params["is_specific_enough"] and len(params["keywords"]) >= 1:
        params["is_specific_enough"] = True
    elif not params["is_specific_enough"] and not params["keywords"]:
        params["is_specific_enough"] = False
    return params

# --- ESTA ES LA VERSI√ìN DE find_relevant_chunks QUE SE CONSERVA Y CORRIGE ---
def find_relevant_chunks(question, all_docs_chunks, max_chunks=10):
    """
    Encuentra chunks relevantes:
    1. Extrae par√°metros (a√±o, REGIONES) de la pregunta.
    2. Pre-filtra chunks basados en estos par√°metros.
    3. Calcula un score de relevancia para los chunks pre-filtrados basado en palabras clave.
    4. Devuelve un diccionario con el estado y los chunks.
    """
    # --- CORRECCI√ìN AQU√ç: Llama a la funci√≥n extract_query_parameters definida arriba ---
    query_params = extract_query_parameters(question)

    if not query_params["is_specific_enough"]:
        return {"needs_more_specificity": True, "chunks": []}

    pre_filtered_chunks = []
    apply_pre_filtering = bool(query_params["years"] or query_params["regions"])

    if apply_pre_filtering:
        for chunk in all_docs_chunks:
            metadata = chunk.get("metadata", {})
            year_match = True
            if query_params["years"]:
                year_meta_str = str(metadata.get("year", "")).strip()
                year_match = year_meta_str in query_params["years"]

            region_match = True
            if query_params["regions"]:
                region_meta_norm = normalize_text(metadata.get("region", ""))
                region_match = any(q_reg == region_meta_norm for q_reg in query_params["regions"])

            if year_match and region_match:
                pre_filtered_chunks.append(chunk)

        if not pre_filtered_chunks:
            return {"needs_more_specificity": False, "chunks": [], "no_data_for_filter": True, "params": query_params}
    else:
        pre_filtered_chunks = all_docs_chunks

    if not pre_filtered_chunks:
         return {"needs_more_specificity": False, "chunks": []}

    relevance_scores = []
    question_norm_keywords = set(query_params["keywords"])

    if not question_norm_keywords and apply_pre_filtering and pre_filtered_chunks:
        return {"needs_more_specificity": False, "chunks": pre_filtered_chunks[:max_chunks]}

    if not question_norm_keywords and not apply_pre_filtering:
        return {"needs_more_specificity": True, "chunks": []}

    for chunk_idx, chunk in enumerate(pre_filtered_chunks):
        metadata = chunk.get("metadata", {})
        chunk_text_norm = normalize_text(chunk.get("chunk_text", ""))
        titulo_norm = normalize_text(metadata.get("titulo_informe", ""))
        entidad_norm = normalize_text(metadata.get("entidad_auditada", ""))

        combined_text_for_scoring = f"{chunk_text_norm} {titulo_norm} {entidad_norm}"
        chunk_keywords = set(re.findall(r'\b[a-z]{3,}\b', combined_text_for_scoring))

        common_keywords = question_norm_keywords.intersection(chunk_keywords)
        score = len(common_keywords)

        source_field = chunk.get("source_field", "")
        if source_field == "observacion" and any(kw in question_norm_keywords for kw in ["corrupcion", "irregularidad", "hallazgo", "perjuicio", "delito"]):
            score += 5
        if source_field == "objetivo" and "objetivo" in question_norm_keywords: score += 3
        if source_field == "recomendacion" and any(kw in question_norm_keywords for kw in ["recomienda", "sugiere", "recomendacion"]): score += 3

        if query_params["regions"]:
            if any(q_reg in chunk_text_norm for q_reg in query_params["regions"]):
                score += 1

        relevance_scores.append({"score": score, "chunk": chunk, "original_index": chunk_idx})

    relevant_chunks_sorted = sorted(relevance_scores, key=lambda x: (x["score"], -x["original_index"]), reverse=True)

    if not question_norm_keywords and apply_pre_filtering and pre_filtered_chunks:
         final_chunks = [item["chunk"] for item in relevant_chunks_sorted][:max_chunks]
    else:
        final_chunks = [item["chunk"] for item in relevant_chunks_sorted if item["score"] > 0][:max_chunks]

    if not final_chunks and apply_pre_filtering:
        return {"needs_more_specificity": False, "chunks": [], "no_data_for_filter_after_score": True, "params": query_params}

    return {"needs_more_specificity": False, "chunks": final_chunks}


def send_question_to_openai(question, all_docs_chunks, conversation_history):
    retrieval_result = find_relevant_chunks(question, all_docs_chunks, max_chunks=15)

    if retrieval_result.get("needs_more_specificity"):
        return "Por favor, proporciona m√°s detalles en tu consulta, como un a√±o espec√≠fico o regi√≥n, para poder ayudarte mejor." # Mensaje ajustado

    relevant_chunks = retrieval_result.get("chunks", [])

    if not relevant_chunks:
        if retrieval_result.get("no_data_for_filter") or retrieval_result.get("no_data_for_filter_after_score"):
            params = retrieval_result.get("params", {})
            year_str = ", ".join(params.get("years", [])) or "el per√≠odo consultado"
            # Ahora solo hay regiones en params
            loc_parts = params.get("regions", [])
            loc_str = ", ".join(loc_parts) or "la localidad consultada"
            if params.get("years") or loc_parts:
                 return f"No encontr√© informes que coincidan exactamente con tu consulta para {loc_str} en {year_str}. Intenta con otros par√°metros o consulta directamente a la Contralor√≠a General de la Rep√∫blica del Per√∫."
        return "No dispongo de informaci√≥n espec√≠fica para tu consulta. Por favor, intenta reformularla o consulta directamente a la Contralor√≠a General de la Rep√∫blica del Per√∫."

    context_text = "\n\n---\n\n".join([
        f"Del Informe: {chunk['metadata'].get('numero_informe', 'N/A')}\n"
        f"Entidad Auditada: {chunk['metadata'].get('entidad_auditada', 'N/A')}\n"
        f"A√±o del Informe: {chunk['metadata'].get('year', 'N/A')}\n"
        f"Regi√≥n: {chunk['metadata'].get('region', 'N/A')}\n"
        # Se eliminaron provincia y distrito del contexto ya que no se usan en el filtrado principal
        f"Tipo de Informaci√≥n (Chunk): {chunk.get('source_field', 'N/A')}\n"
        f"Texto del Chunk:\n{chunk.get('chunk_text', '')}"
        for chunk in relevant_chunks
    ])

    MAX_HISTORY_MESSAGES = 10
    trimmed_history = conversation_history[-MAX_HISTORY_MESSAGES:]
    messages = []
    combined_system_prompt = f"{system_prompt_v2}\n\nContexto relevante de los informes:\n{context_text}"
    messages.append({"role": "system", "content": combined_system_prompt})
    messages.extend(trimmed_history)
    messages.append({"role": "user", "content": question})

    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo", # Cambiado a un modelo m√°s com√∫n si gpt-4.1-nano no existe o es privado
            messages=messages,
            temperature=0,
            max_tokens=1500,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0
        )
        return response.choices[0].message.content
    except Exception as e:
        st.error(f"Error al contactar OpenAI: {e}")
        return "Lo siento, tuve un problema al procesar tu solicitud en este momento."


def main():
    st.title("Chatbot Corrupci√≥n üí¨")
    st.markdown("Conversa con los informes de la contralor√≠a sobre corrupci√≥n en gobiernos subnacionales en Per√∫ (2016-2022).")
    st.write("---")

    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "Hola, soy el Chatbot Corrupci√≥n. ¬øEn qu√© puedo ayudarte?"}]

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if user_input := st.chat_input("Escribe tu pregunta aqu√≠..."):
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        with st.spinner("Generando respuesta..."):
            # Prepara el historial de mensajes para la API (excluyendo el mensaje del sistema y la entrada actual)
            # El system prompt se a√±ade dentro de send_question_to_openai
            conversation_history = [
                msg for msg in st.session_state.messages[:-1] if msg["role"] != "system"
            ]
            response_text = send_question_to_openai(user_input, docs_chunks, conversation_history)

            assistant_message = {"role": "assistant", "content": response_text}
            st.session_state.messages.append(assistant_message)
            # Para mostrar la respuesta inmediatamente
            with st.chat_message("assistant"):
                 st.markdown(response_text)
            # No necesitas st.experimental_rerun() aqu√≠ a menos que quieras forzar un rerun por otra raz√≥n.
            # El chat_message ya actualiza la UI.

if __name__ == "__main__":
    main()
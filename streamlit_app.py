import os
import json
import streamlit as st
import openai
from dotenv import load_dotenv
import re
from openai import OpenAI
import geopandas as gpd
import folium
from streamlit_folium import st_folium
import unicodedata

client = OpenAI(
  api_key=st.secrets['openai_key'],  # Poner Key
)

# Configuraci√≥n de Streamlit
st.set_page_config(page_title="Chatbot Corrupci√≥n üí¨", layout="centered")

with st.sidebar:
    st.image(".streamlit/logo.png", use_container_width=True)
    st.title('Chatbot Corrupci√≥n')
    st.markdown('''
    ## Sobre este Chatbot

    Bienvenido al **Chatbot Corrupci√≥n**, una herramienta interactiva dise√±ada para facilitar el acceso y comprensi√≥n de los informes de auditor√≠a relacionados con casos de corrupci√≥n en los gobiernos subnacionales del Per√∫ durante el per√≠odo **2016-2022**.

    Nuestra base de datos incluye **todos los informes de control** emitidos por la **Contralor√≠a General de la Rep√∫blica del Per√∫** en esos a√±os, proporcionando una cobertura completa y actualizada de las acciones de control realizadas a nivel nacional.

    Utiliza este chatbot para explorar informaci√≥n detallada sobre auditor√≠as, hallazgos y recomendaciones, y para obtener respuestas precisas basadas en los documentos oficiales.

    ---
    ### C√≥mo utilizar este Chatbot
    - **Realiza preguntas claras y espec√≠ficas** sobre los informes de auditor√≠a.
    - **Indica localidades o per√≠odos de inter√©s** para obtener informaci√≥n detallada.
    - **Recuerda que las respuestas se basan en documentos oficiales**, y si no se dispone de cierta informaci√≥n, se te proporcionar√° orientaci√≥n para obtenerla.

    ---
    ''')
    st.markdown('Desarrollado por **Q-Lab** - Laboratorio de Inteligencia Artificial y M√©todos Computacionales en Ciencias Sociales ([qlab.pucp.edu.pe](https://qlab.pucp.edu.pe/))')
    st.markdown('Contacto: ‚úâÔ∏è [qlab_csociales@pucp.edu.pe](mailto:qlab_csociales@pucp.edu.pe)')

    if st.button("üóëÔ∏è Limpiar conversaci√≥n"):
        st.session_state.messages = [{"role": "assistant", "content": "Conversaci√≥n reiniciada. ¬øEn qu√© m√°s puedo ayudarte?"}]
        st.experimental_rerun()

load_dotenv()

def load_chunks_from_json(input_file='data/processed/processed_data.json'):
    with open(input_file, 'r', encoding='utf-8') as f:
        docs_chunks = json.load(f)
    return docs_chunks

docs_chunks = load_chunks_from_json('data/processed/processed_data.json')

system_prompt = """
Eres un experto en informes de auditor√≠a sobre corrupci√≥n en los gobiernos subnacionales de Per√∫. Coloquialmente, la gente se referir√° a estos como informes de corrupci√≥n, o informaci√≥n sobre corrupci√≥n. Responde a las preguntas bas√°ndote √∫nicamente en los datos de los documentos proporcionados (Informes de Servicios de Control) de la Contralor√≠a General de la Rep√∫blica del Per√∫.

Al elaborar tus respuestas:

- Ten en cuenta que todos los informes que tienes provienen de auditor√≠as. Aunque estos no sean expl√≠citamente 'corrupci√≥n', analizan casos de sospecha de corrupci√≥n. Por lo tanto, si alguien te pregunta sobre un caso de corrupci√≥n, puedes utilizar los informes para responderle. 
- Proporciona informaci√≥n completa, precisa y √∫til basada en los documentos disponibles.
- Cuando utilices informaci√≥n espec√≠fica de un documento, menciona al inicio el n√∫mero de informe de donde proviene. Por ejemplo: "Seg√∫n el informe '002-2017-2-5510', se encontr√≥ que..."
- Si se te pregunta sobre corrupci√≥n en una localidad y/o per√≠odo espec√≠fico, y no tienes informaci√≥n exacta, indica qu√© informaci√≥n relacionada tienes disponible y proporciona todos los detalles relevantes de los informes que posees. Por ejemplo: "No dispongo de informaci√≥n sobre Chiclayo en 2017, pero s√≠ del 2014. Seg√∫n el informe 'XXX-XXXX-XXXX', se encontr√≥ que...". Si tienes informaci√≥n de varios per√≠odos, menci√≥nalos todos y detalla los hallazgos.
- Si te piden m√°s detalles sobre un informe en particular, proporciona toda la informaci√≥n disponible de ese informe sin omitir detalles relevantes.
- Siempre que tengas informaci√≥n adicional relevante, ofr√©cela al usuario sin esperar a que te lo solicite.
- Si no conoces la respuesta a una pregunta o no tienes informaci√≥n al respecto, responde: "No dispongo de esa informaci√≥n, por favor consulte la Contralor√≠a General de la Rep√∫blica del Per√∫."
"""

def main():
    st.title("Chatbot Corrupci√≥n üí¨")
    st.markdown("Conversa con los informes de la contralor√≠a sobre corrupci√≥n en gobiernos subnacionales en Per√∫ (2016-2022).")
    st.write("---")  # L√≠nea divisoria

    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "Hola, soy el Chatbot Corrupci√≥n. ¬øEn qu√© puedo ayudarte?"}]

    # Mostrar los mensajes previos
    for message in st.session_state.messages:
        if message["role"] == "assistant":
            with st.chat_message("assistant"):
                st.markdown(message["content"])
        elif message["role"] == "user":
            with st.chat_message("user"):
                st.markdown(message["content"])

    # Capturar la entrada del usuario
    if user_input := st.chat_input("Escribe tu pregunta aqu√≠..."):
        user_message = {"role": "user", "content": user_input}
        st.session_state.messages.append(user_message)

        with st.chat_message("user"):
            st.markdown(user_input)

        with st.spinner("Generando respuesta..."):
            # Prepara el historial de mensajes para la API (excluyendo el mensaje del sistema)
            conversation_history = st.session_state.messages[:-1]  # Excluye la entrada actual
            response_text = send_question_to_openai(user_input, docs_chunks, conversation_history)
            if response_text:
                assistant_message = {"role": "assistant", "content": response_text}
                st.session_state.messages.append(assistant_message)

                with st.chat_message("assistant"):
                    st.markdown(response_text)
            else:
                st.error("No se pudo obtener una respuesta.")

def find_relevant_chunks(question, docs_chunks, max_chunks=5):
    question_keywords = set(re.findall(r'\w+', question.lower()))
    relevance_scores = []

    for chunk in docs_chunks:
        # Combina el t√≠tulo y el contenido para la comparaci√≥n
        combined_text = (chunk["title"] + " " + chunk["content"]).lower()
        chunk_keywords = set(re.findall(r'\w+', combined_text))
        common_keywords = question_keywords.intersection(chunk_keywords)
        relevance_scores.append((len(common_keywords), chunk))

    relevant_chunks = [chunk for _, chunk in sorted(relevance_scores, key=lambda x: x[0], reverse=True)]
    return relevant_chunks[:max_chunks]

def send_question_to_openai(question, docs_chunks, conversation_history):
    # Encuentra los chunks m√°s relevantes para la pregunta
    relevant_chunks = find_relevant_chunks(question, docs_chunks)

    # Construye el contexto incluyendo el t√≠tulo y el contenido de cada chunk
    context_text = "\n\n".join([
        f"T√≠tulo del Documento: {chunk['title']}\nContenido:\n{chunk['content']}"
        for chunk in relevant_chunks
    ])

    # Limita el historial a los √∫ltimos N mensajes para controlar el n√∫mero de tokens
    MAX_HISTORY_MESSAGES = 5  # Puedes ajustar este n√∫mero seg√∫n tus necesidades
    trimmed_history = conversation_history[-MAX_HISTORY_MESSAGES:]

    # Construye la lista de mensajes para la API
    messages = []

    # A√±ade el mensaje del sistema combinado con el contexto relevante
    combined_system_prompt = f"{system_prompt}\n\nContexto relevante:\n{context_text}"
    messages.append({"role": "system", "content": combined_system_prompt})

    # A√±ade el historial de conversaci√≥n previo
    messages.extend(trimmed_history)

    # A√±ade la pregunta actual del usuario
    messages.append({"role": "user", "content": question})

    # Llama a la API de OpenAI con los mensajes actualizados
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=0,
        max_tokens=1024,  # Ajusta seg√∫n sea necesario
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0
    )

    # Devuelve la respuesta generada
    return response.choices[0].message.content

if __name__ == "__main__":
    main()
